# city_scape_neural_network

## Анализ стилистических особенностей городских ландшафтов с помощью многослойных нейронных сетей

***

Система, обучающаяся на некотором наборе изображений и позволяющая выполнить модификацию произвольного изображения в соответствии со стилем выбранной группы городов

***

### Составление выборки

В папке ***data*** представлена выборка изображений из 5 классов типичных городских ландшафтов. Использовались виды Москвы, Дубая, Праги, Венеции и обобщенный класс для старой японской архитектуры.
Представлено 500 изображений каждого класса для обучения и 200 для проверки. Это достаточно небольшие объемы и будет проведено сравнение методов обучения на них.

Для ее составления использовался скрипт ***bing.py*** использующий Bing Search API, представленный в папке ***/bingsearch***

### Создание модели

Использованы 2 метода для создания классификатора изображений:
- Обучение небольшой сети с нуля
- Использование возможностей предварительно подготовленной сети

#### Сеть с нуля. ***classifier_net_trained_from_scratch.ipynb***

Поскольку выборка маленькая существует проблема переобучения. Переобучение происходит, когда модели представленно слишком мало примеров, то есть она изучает шаблоны, которые не обобщаются на новые данные и модель начинает использовать нерелевантные функции для создания прогнозов.

Увеличение данных - один из способов борьбы с переобучением, но этого недостаточно, поскольку наши расширенные образцы все еще сильно коррелированы. Основной упор в борьбе с переобучением должен быть энтропийный потенциал вашей модели - сколько информации ваша модель может хранить. Модель, которая может хранить много информации, потенциально может быть более точной за счет использования большего количества функций, но она также более подвержена риску хранения нерелевантные функции. Между тем, модель, которая может хранить только несколько функций, должна будет сосредоточиться на наиболее важных функциях, имеющихся в данных, и они, скорее всего, будут действительно актуальны и лучше обобщены.

Существуют различные способы модуляции энтропийной способности. Главное - это выбор количества параметров модели, то есть количество слоев и размер каждого слоя. Другим способом является использование регуляции веса, которая заключается в принуждении весов модели принимать меньшие значения.

Приведен фрагмент кода первой модели, простой стек из трех слоев свертки с активацией ReLU и последующими слоями с максимальным объединением (похоже на архитектуры, которые Yann LeCun пропагандировал в 1990-х годах для классификации изображений).

#### Сеть построенная на предварительно обученной модели. ***classifier_net_used_ pre-trained_model.ipynb***

Более совершенный подход состоит в том, чтобы использовать сеть, предварительно подготовленную для большого набора данных. Такая сеть уже узнала бы функции, полезные для большинства проблем с компьютерным зрением, и использование таких функций позволило бы достичь более высокой точности, чем любой метод, который будет опираться только на имеющиеся данные.

Используется архитектура VGG16, предварительно подготовленная к набору данных ImageNet. Стратегия такова: мы создаем только сверточную часть модели, все до полностью связанных уровней. Затем мы запускаем эту модель на наших данных по тренировке и валидации один раз, записав результат в двух массивах numpy. Затем мы обучим небольшую полностью связанную модель поверх сохраненных функций.

Причина, по которой мы сохраняем функции в автономном режиме, а не добавляем нашу полностью связанную модель непосредственно поверх замороженной сверточной базы и запуска всего этого, - это вычислительная эффективность.

[Сайт Visual Geometry Group](http://www.robots.ox.ac.uk/~vgg/research/very_deep/)


[VGG-16 для Keras](https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3)